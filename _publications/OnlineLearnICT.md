---
title: "Online-Learning-Based Human Tracking Across Non-Overlapping Cameras"
collection: publications
permalink: /publication/OnlineLearnICT
<!-- excerpt: 'This paper is about the number 1. The number 2 is left for future work.' -->
date: 2017-05-23
venue: "IEEE Transactions on Circuits and Systems for Video Technology"
paperurl: "http://ieeexplore.ieee.org/document/7932896"
citation: 'Young-Gun Lee, <b>Zheng Tang</b> and Jenq-Neng Hwang. "Online-Learning-Based Human Tracking Across Non-Overlapping Cameras". <i>IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)</i>. vol. 28, no. 10, pp. 2870-2883. 2018.'
---
# Online-Learning-Based Human Tracking Across Non-Overlapping Cameras

[<a href="https://ieeexplore.ieee.org/document/7932896">Paper</a>]


## Abstract
Due to the expanding scale of camera networks, Multiple Camera Tracking (MCT) of humans has received increased attention in recent years. In this paper, we present a novel approach to tracking each human within a single camera and across multiple disjoint cameras. Our framework includes a multi-object tracking and segmentation system, a two-phase feature extractor, and an online-learning-based camera link model estimation. For tracking within a single camera, we apply tracking by segmentation and local object detection with multi-kernel feedback to adaptively improve the robustness of the algorithm. In inter-camera tracking, we introduce an effective integration of appearance and context features. Couples are automatically detected, and the couple feature is also integrated with existing features. The proposed algorithm is scalable by a fully unsupervised online-learning framework. In our experiments, the proposed method outperforms all of the state-of-the-art methods in the benchmark National Laboratory of Pattern Recognition NLPR_MCT dataset.


## Citation
@article{Lee18OnlineLearnICT,  
author = {Young-Gun Lee and Zheng Tang and Jenq-Neng Hwang},  
title = {Online-learning-based human tracking across non-overlapping cameras},  
journal = {T-CSVT},  
volume = {28},  
number = {10},  
pages = {2870--2883},  
year = {2018}  
}
