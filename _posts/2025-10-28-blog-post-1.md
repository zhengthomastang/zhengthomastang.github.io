---
title: "Vision AI Agents for Foxconn Digital Twins Featured at NVIDIA GTC DC 2025 & Hosting the 9th AI City Challenge at ICCV 2025"
excerpt: "Our vision AI agents for Foxconn’s factory digital twin were showcased in Jensen Huang’s GTC DC 2025 keynote, alongside hosting the 9th AI City Challenge Workshop at ICCV 2025 with 245 global teams advancing multi-camera 3D perception and spatial AI."
date: 2025-10-28
permalink: /posts/2025/10/blog-post-1/
tags:
  - work
  - research
---

I’m honored that our work was featured in [**Jensen Huang’s keynote at NVIDIA GTC DC 2025**](https://www.nvidia.com/gtc/dc/keynote/), where we showcased a new generation of **vision AI agents** operating inside a **Foxconn factory digital twin**.  
Built on **NVIDIA Metropolis**, **Cosmos**, and **Omniverse**, these agents monitor large industrial spaces from an overhead perspective, continuously analyzing activity, detecting anomalies, and assisting Foxconn engineers with real-time safety and operational insights.

This demo illustrates the future of **Physical AI**: unifying simulation, multimodal perception, and large-scale intelligence to power safer, more efficient, and more autonomous industrial environments. Grateful to our teammates across **Metropolis**, **Robotics**, and **Omniverse** who made this possible.

In parallel, I was also honored to represent **NVIDIA Metropolis** as host of the [**9th AI City Challenge Workshop**](https://www.aicitychallenge.org/) at [**ICCV 2025**](https://iccv.thecvf.com/) in Honolulu. This year, the challenge brought together **245 teams from 15 countries**, pushing forward research in:

- Multi-camera 3D perception  
- Traffic safety reasoning  
- Warehouse spatial intelligence  
- Edge-optimized fisheye detection  

The challenge datasets—including [PhysicalAI-SmartSpaces](https://huggingface.co/datasets/nvidia/PhysicalAI-SmartSpaces) and [WTS Traffic Safety](https://woven-visionai.github.io/wts-dataset-homepage/)—were downloaded **nearly 400,000 times on Hugging Face**, reflecting the global momentum behind large-scale spatial AI benchmarks.

Additionally, I was invited as a keynote speaker at the [**8th Workshop on Benchmarking Multi-Target Tracking (BMTT)**](https://motchallenge.net/workshops/bmtt2025/), where I presented NVIDIA’s recent advances in:

- [**MCBLT (BEV-SUSHI)**](https://arxiv.org/abs/2412.00692): hierarchical GNN-based multi-camera 3D tracking  
- [**Sparse4D**](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/sparse4d_rn101): end-to-end multi-camera 3D perception  
- Integration with [**SUSHI GNN**](https://github.com/dvl-tum/SUSHI) for scalable, long-range association  

Special thanks to collaborators **Yizhou Wang**, **Sameer Pusegaonkar**, and our partners in **Laura Leal-Taixé’s group**, whose contributions continue to push the boundaries of multi-camera 3D understanding.

Congratulations to all participants, winners, reviewers, and organizers who made this year’s workshops a success. Looking forward to driving the next wave of **Spatial AI**, **digital twins**, and **large-space perception** together with the community.

<p align="center">
  <img src="https://zhengthomastang.github.io/images/AICity25_photo.jpg" alt="9th AI City Challenge Workshop at ICCV 2025" style="width: 750px;"/>
</p>
